{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9f8a23a5-df60-4a53-9ba5-a796ff08ba3b",
   "metadata": {},
   "source": [
    "## 데이터셋 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3616ba85-4be5-44f5-ac05-e17a70522409",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 12:17:11.358705: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-11-06 12:17:11.615364: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-06 12:17:12.204996: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e471d562-5400-4e81-a5d5-5bb267796fba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 16000 files belonging to 10 classes.\n",
      "Found 2000 files belonging to 10 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_ShuffleDataset element_spec=(TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 10), dtype=tf.float32, name=None))>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# config\n",
    "root_dir = \"/mnt/d/Exa/227.건설 현장 위험 상태 판단 데이터/01-1.정식개방데이터/Minisample/\"\n",
    "makeset = tf.keras.utils.image_dataset_from_directory\n",
    "class_type = [\"binary\", \"categorical\"][1]\n",
    "\n",
    "# make dataset\n",
    "batch_size = 64\n",
    "image_size = (224, 224)\n",
    "seed = 9140183 # random number\n",
    "train_set = makeset(\n",
    "    f\"{root_dir}train\",\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    seed=seed,\n",
    "    label_mode=class_type\n",
    ")\n",
    "valid_set = makeset(\n",
    "    f\"{root_dir}validation\",\n",
    "    image_size=image_size,\n",
    "    batch_size=batch_size,\n",
    "    seed=seed,\n",
    "    label_mode=class_type\n",
    ")\n",
    "train_set.shuffle(buffer_size=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ceffa007-8ac0-4b40-91cd-3a46ce4dc722",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch size: (64, 224, 224, 3) / label batch size: (64, 10)\n"
     ]
    }
   ],
   "source": [
    "# check dataset\n",
    "category_num = 0\n",
    "for d, l in train_set:\n",
    "    _, category_num = l.shape\n",
    "    print(f\"data batch size: {d.shape} / label batch size: {l.shape}\")\n",
    "    break\n",
    "if category_num == 1:\n",
    "\tassert class_type == \"binary\", \"분류 설정 오류\"\n",
    "elif category_num >= 2:\n",
    "\tassert class_type == \"categorical\", \"분류 설정 오류\"\n",
    "else:\n",
    "\traise Exception(\"검증 데이터셋 오류\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d899febe-88a4-4a39-bf27-77279f205371",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ref_model = tf.keras.applications.EfficientNetB0(weights=None) # 처음부터 다시 훈련: weights=None\n",
    "base_model = tf.keras.models.Model(\n",
    "\tinputs=ref_model.input,\n",
    "\toutputs=ref_model.layers[-2].output\n",
    ")\n",
    "\n",
    "x = base_model.output\n",
    "if class_type == \"binary\":\n",
    "    predictions = layers.Dense(1, activation = \"sigmoid\")(x)\n",
    "else:\n",
    "    predictions = layers.Dense(category_num, activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)\n",
    "loss_function = f\"{class_type}_crossentropy\"\n",
    "model.compile(loss=loss_function, optimizer=\"rmsprop\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3bfce2fa-9b90-4c67-a7bd-a78d14b3e409",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m 99/250\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 150ms/step - accuracy: 0.8754 - loss: 0.2964"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Corrupt JPEG data: premature end of data segment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m250/250\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step - accuracy: 0.8973 - loss: 0.2591"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-06 14:19:00.637632: E tensorflow/core/lib/jpeg/jpeg_mem.cc:327] Premature end of JPEG data. Stopped at line 144/1080\n",
      "2024-11-06 14:19:00.721160: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: jpeg::Uncompress failed. Invalid JPEG data or crop window.\n",
      "\t [[{{node decode_image/DecodeImage}}]]\n",
      "\t [[IteratorGetNext]]\n",
      "2024-11-06 14:19:00.721308: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: INVALID_ARGUMENT: jpeg::Uncompress failed. Invalid JPEG data or crop window.\n",
      "\t [[{{node decode_image/DecodeImage}}]]\n",
      "\t [[IteratorGetNext]]\n",
      "\t [[IteratorGetNext/_2]]\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node decode_image/DecodeImage defined at (most recent call last):\n<stack traces unavailable>\nDetected at node decode_image/DecodeImage defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  jpeg::Uncompress failed. Invalid JPEG data or crop window.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) INVALID_ARGUMENT:  jpeg::Uncompress failed. Invalid JPEG data or crop window.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_one_step_on_iterator_213828]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# benchmark\u001b[39;00m\n\u001b[1;32m      2\u001b[0m start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m----> 3\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalid_set\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTime spent: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mint\u001b[39m((end\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m60\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mm \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m(end\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart)\u001b[38;5;250m \u001b[39m\u001b[38;5;241m%\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m60\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     54\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node decode_image/DecodeImage defined at (most recent call last):\n<stack traces unavailable>\nDetected at node decode_image/DecodeImage defined at (most recent call last):\n<stack traces unavailable>\n2 root error(s) found.\n  (0) INVALID_ARGUMENT:  jpeg::Uncompress failed. Invalid JPEG data or crop window.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]]\n\t [[IteratorGetNext/_2]]\n  (1) INVALID_ARGUMENT:  jpeg::Uncompress failed. Invalid JPEG data or crop window.\n\t [[{{node decode_image/DecodeImage}}]]\n\t [[IteratorGetNext]]\n0 successful operations.\n0 derived errors ignored. [Op:__inference_one_step_on_iterator_213828]"
     ]
    }
   ],
   "source": [
    "# benchmark\n",
    "start = time.time()\n",
    "history = model.fit(\n",
    "    train_set,\n",
    "    epochs=5,\n",
    "    validation_data=valid_set\n",
    ")\n",
    "end = time.time()\n",
    "print(f\"Time spent: {int((end - start) // 60)}m {(end - start) % 60:.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "730a408f-67dc-4313-8246-66c2473c88a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# plot\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(\u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      3\u001b[0m loss \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      4\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "# plot\n",
    "epochs = range(1, len(history.history[\"accuracy\"]) + 1)\n",
    "loss = history.history[\"accuracy\"]\n",
    "val_loss = history.history[\"val_accuracy\"]\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation accuracy\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2884d84e-d522-4510-99ed-3aa80670831c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 4s/step\n",
      "Predicted class: [0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(224, 224))\n",
    "    img_array = img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    return img_array\n",
    "\n",
    "processed_img = preprocess_image(\"/mnt/d/ChromeDownloads/wheelbarrow-lowering-400.jpg\")\n",
    "prediction = model.predict(processed_img)\n",
    "predicted_class = np.argmax(prediction, axis=1)\n",
    "print(f\"Predicted class: {predicted_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d403ba22-0272-4f5c-b307-c11f97b43e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
